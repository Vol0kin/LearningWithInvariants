@Article{Vapnik2019,
author={Vapnik, Vladimir
and Izmailov, Rauf},
title={Rethinking statistical learning theory: learning using statistical invariants},
journal={Machine Learning},
year={2019},
month={Mar},
day={01},
volume={108},
number={3},
pages={381-423},
abstract={This paper introduces a new learning paradigm, called Learning Using Statistical Invariants (LUSI), which is different from the classical one. In a classical paradigm, the learning machine constructs a classification rule that minimizes the probability of expected error; it is data-driven model of learning. In the LUSI paradigm, in order to construct the desired classification function, a learning machine computes statistical invariants that are specific for the problem, and then minimizes the expected error in a way that preserves these invariants; it is thus both data- and invariant-driven learning. From a mathematical point of view, methods of the classical paradigm employ mechanisms of strong convergence of approximations to the desired function, whereas methods of the new paradigm employ both strong and weak convergence mechanisms. This can significantly increase the rate of convergence.},
issn={1573-0565},
doi={10.1007/s10994-018-5742-0},
url={https://doi.org/10.1007/s10994-018-5742-0}
}

@article{Vapnik2015,
author = {Vapnik, Vladimir and Izmailov, Rauf},
title = {V-Matrix Method of Solving Statistical Inference Problems},
year = {2015},
issue_date = {January 2015},
publisher = {JMLR.org},
volume = {16},
number = {1},
issn = {1532-4435},
abstract = {This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also defined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called V-matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1683–1730},
numpages = {48},
keywords = {regression, data adaptation, conditional probability, reproducing kernel Hilbert space, function estimation, conditional density, mutual information, interpolation function, support vector machines, density ratio, data balancing, ill-posed problem}
}

@inproceedings{Dasgupta2000,
author = {Dasgupta, Sanjoy},
title = {Experiments with Random Projection},
year = {2000},
isbn = {1558607099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recent theoretical work has identified random projection as a promising dimensionality reduction technique for learning mixtures of Gaussians. Here we summarize these results and illustrate them by a wide variety of experiments on synthetic and real data.},
booktitle = {Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
pages = {143–151},
numpages = {9},
location = {Stanford, California},
series = {UAI'00}
}

@inproceedings{BinghamManila2001,
author = {Bingham, Ella and Mannila, Heikki},
title = {Random Projection in Dimensionality Reduction: Applications to Image and Text Data},
year = {2001},
isbn = {158113391X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502512.502546},
doi = {10.1145/502512.502546},
abstract = {Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.},
booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {245–250},
numpages = {6},
keywords = {dimensionality reduction, image data, text document data, high-dimensional data, random projection},
location = {San Francisco, California},
series = {KDD '01}
}

@article{DietrichBakiri1995,
  added-at = {2020-01-10T00:00:00.000+0100},
  author = {Dietterich, Thomas G. and Bakiri, Ghulum},
  biburl = {https://www.bibsonomy.org/bibtex/2c90ac39019dd62c4a663cd80bc2e539e/dblp},
  ee = {https://arxiv.org/abs/cs/9501101},
  interhash = {ddf6a7f65b2ee36f3d32648b7c3cac74},
  intrahash = {c90ac39019dd62c4a663cd80bc2e539e},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2020-01-11T11:39:36.000+0100},
  title = {Solving Multiclass Learning Problems via Error-Correcting Output Codes},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr9501.html#cs-AI-9501101},
  volume = {cs.AI/9501101},
  year = 1995
}

@misc{UCIRepository,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

