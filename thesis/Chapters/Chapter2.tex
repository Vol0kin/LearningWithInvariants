% Chapter Template

\newcommand{\Tau}{\mathcal{T}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\innerprod}[1]{\left< #1 \right>}
\newcommand{\set}[1]{\lbrace #1 \rbrace}

\chapter{Learning using statistical invariants} % Main chapter title
\label{Chapter2}

Given that this work intends to explore the applications of the invariants in the learning
process, we first need to introduce the background work that proposed this new learning
paradigm, which is called LUSI (Learning Using Statistical Invariants).

This chapter intends to provide the necessary background to understand the basis of this work
and an overview of the most relevant aspects of the original paper that presented the LUSI paradigm,
which was proposed by \cite{Vapnik2019}. For further information and more details, please
refer to the original paper.

\section{Weak convergence and the LUSI paradigm}

Supervised machine learning algorithms try to find the best estimate of some conditional probability
function $P(y | x)$, i.e., given a data point $x$, we want to compute the probability that this
point belongs to a particular class $y$.

Classical methods do this by using the strong mode of convergence in the Hilbert space. However,
in the LUSI paradigm this estimation is obtained using the weak mode of convergence. Hence, it is
important to understand the difference between this two modes of convergence and what role
the weak mode of convergence plays in the LUSI paradigm.

\subsection{Strong and weak modes of convergence}

In a Hilbert space, the relationships between two functions $f_1(x)$ and $f_2(x)$ have two
numerical properties:

\begin{enumerate}
    \item The distance between functions
    
    \[
        \rho (f_1, f_2) = \norm{f_1(x) - f_2(x)}
    \]
    
    that is defined by the metric of the $L_2$ space and
    
    \item The inner product between functions
    
    \[
        R(f_1, f_2) = \innerprod{f_1(x), f_2(x)}
    \]
    
    that has to satisfy the corresponding requirements.
\end{enumerate}

These two properties imply two different modes of convergence: a strong one and a weak one. Classical
learning paradigms rely on the strong convergence mode (convergence in metrics), trying to find a
sequence of functions $\set{P_l(y=1 | x)}$\footnote{We focus here in the binary problem setting, i.e. $y\in \{0,1\}$. Thus, $P_l(y=1 | x)$ fully specifies the output probability distribution.} such that

\[
    \lim_{l \to \infty} \norm{P_l(y=1 | x) - P(y=1 | x)} = 0\quad \forall x
\]

The weak mode of convergence (convergence in inner products) is given by

\[
    \lim_{l \to \infty} \innerprod{P_l(y=1 | x) - P(y=1 | x), \psi(x)} = 0\quad \forall \psi(x) \in L_2
\]

Note that this mode of convergence has to take place for \emph{all} functions in the Hilbert space $L_2$.

It is known that the strong mode of convergence implies the weak one, although generally speaking, the
reverse is not true.

\subsection{The LUSI paradigm}

Opposite to the classical learning paradigms, LUSI is based on the weak mode of convergence. It replaces
the infinite set of functions with a set of functions $\mathcal{P} = \set{\psi_1(x), \dots, \psi_m(x)}$
called predicates, which describe some important properties of the desired conditional probability function and
restrict the scope of weak convergence only to the set of functions $\mathcal{P}$. These properties are
called invariants, and can be expressed as the following equalities:

\[
    \int \psi_s P(y=1 | x)dP(x) = \int \psi_s dP(y=1, x) = a_s,\quad s = 1, \dots, m
\]

where $a_s$ is the expected value of the predicate $\psi_s(x)$ with respect to measure
$P(y=1, x)$. These values are unknown but can be estimated using the training data
$\set{(x_i, y_i),\ i = 1, \dots, l}$. Therefore, the previous expression can be rewritten
as follows:

\begin{equation}
    \label{eq:invariant_approximation}
    \frac{1}{l} \sum_{i=1}^l \psi_s(x_i)P_l(y=1 | x_i) \approx a_s \approx \frac{1}{l} \sum_{i=1}^l y_i \psi_s(x_i),\quad
    s = 1, \dots, m
\end{equation}

Simply put, the general idea of the LUSI paradigm is to find an approximation $P_l(y=1|x)$ of the
real conditional probability function in the subset of functions that preserve the invariants
associated to the set of predicates $\mathcal{P}$, reducing effectively the set of candidate functions
to those that satisfy \eqref{eq:invariant_approximation}.

\subsection{Predicate selection}

In order to find this approximation of the conditional probability function, there must exist
some kind of mechanism that allows us to determine which invariants should be used. Luckily,
the authors propose a very simple way to sequentially selecting invariants. Given an approximation
$P_l^m(y=1|x)$ using $m$ invariants and a new predicate $\psi_{m+1}$ which we would to know whether
it should be considered or not. We can compute the following value before adding it:

\begin{equation}
    \label{eq:predicate_selection}
    \Tau = \frac{\left| \sum_{i=1}^l \psi_{m+1}(x_i) P^m_l(y = 1 | x_i) - \sum_{i=1}^l y_i \psi_{m+1}(x_i) \right|}{\sum_{i=1}^l y_i \psi_{m+1}(x_i)}
\end{equation}

If $\Tau \geq \delta$ for some small threshold $\delta$, the new invariant defined by predicate $\psi_{m+1}$
is considered. Otherwise, the expression \eqref{eq:invariant_approximation} is treated as an equality
and the invariant is not considered in the approximation.


\section{Statistical invariants}

A \emph{statistical invariant} is a specific realization of a predicate with statistical meaning.
This means that it captures some sort of statistical information of the data that has to be
conserved when selecting the best approximation of the conditional probability function.

There are different types of statistical invariants, each one of them
providing different information about the data. In this case, the authors have considered
two in particular: the zeroth order and first order moments of the conditional probability
function $P(y=1|x)$. We will briefly discuss each one of them and see what kind of information
they provide.

\subsection{Zeroth order invariant}

Suppose that we are given a binary classification problem, in which the positive instances
are labeled as 1 and the negative ones as 0. The zeroth order invariant would give us information
about the ratio of elements of the positive class. It is defined as follows:

\[
    \psi_0(x) = 1
\]

The logic behind it is the following: the predicate is applied to each single sample in the dataset,
which will yield the vector $(1, \dots, 1) \in \mathbb{R}^l $, where $l$ is the number of samples. Taking into account
expression \eqref{eq:invariant_approximation}, we can see that each element of this vector is multiplied
by the predicted labels (left side) and the true labels (right side). These values are summed and then divided
by $l$, which gives us the proportion of positive predicted elements on the left side and the proportion
of true positive elements on the right side. Notice that the invariant is only taken into account for those
elements whose predicted or true label is positive. Thus, the negative samples are not considered.

\subsection{First order invariant}

Suppose the same case scenario as in the previous subsection. If we apply the first order invariant to
a dataset, we would get the mean or centroid of the positive class. Its mathematical expression is:

\[
    \psi_1(x) = x
\]

Same as before, when this predicate is applied to the dataset it will generate the vector
$(x_1, \dots, x_l) \in \mathbb{R}^l$. Following expression \eqref{eq:invariant_approximation} again,
only the positive true or predicted elements will be considered. Therefore, their values will be summed and
then averaged, yielding indeed the centroid of the positive class (both for the predicted and true labels).

\section{Solving the learning problem}
\label{sect:solvin_learning_problem}

The authors show that in a specific type of Hilbert space called Reproducing Kernel Hilbert Space (RKHS)
the estimate of the conditional probability function can be computed as

\[
    f(x) = A^T \mathcal{K}(x) + c
\]

where $A \in \mathbb{R}^l$ is a vector of coefficients, $\mathcal{K}(x) = (K(x_1, x), \dots, K(x_l, x))^T$
is a vector of functions determined by the kernel associated to the RKHS\footnote{In this case, the authors have
considered the Gaussian Kernel, which is defined as
\[
    K(x, x') = \exp \lbrace -\delta \norm{x - x'}^2 \rbrace,\; \delta > 0
\]
}
and evaluated on the training data, and $c \in \mathbb{R}$ is the bias term.

Additionally, let $Y = (y_1, \dots, y_l)$  be the labels of the training set,  $K \in \mathbb{R}^{l \times l}$
the matrix with elements $K(x_i, x_j),\; i, j = 1, \dots, l$, $\Phi_s = (\psi_s(x_1), \dots, \psi_s(x_l))^T$
the vector obtained from evaluating the $l$ points of the sample using predicate $\psi_s$,
$1_l = (1, \dots, 1) \in \mathbb{R}^l$ a vector of ones and $V \in \mathbb{R}^{l \times l}$ a matrix called
the $V$-matrix, proposed by \cite{Vapnik2015}, which captures some geometric properties of the data\footnote{The
most simple case considers that the $V$-matrix is equivalent to the identity matrix. Also, the authors found
that using the $V$-matrix over the identity matrix didn't improve the results that much, but it was rather the
use of invariants that made the difference. For the sake of consistency, we are going to keep the $V$-matrix in
the expressions as this is how they are supposed to be written, although bear in mind that it can be substituted with
the identity matrix.}.

With all of this information, we can formulate and solve a minimization problem subject to the constraints
\eqref{eq:predicate_selection} which has a closed-form solution. Using its Lagrangian, we can obtain that the
coefficients $A$ are given by:

\[
    A = (A_V - cA_c) - \left( \sum_{s=1}^m \mu_s A_s \right)
\]

where

\begin{equation*}
    \begin{gathered}
        A_V = (VK + \gamma I)^{-1} VY \\
        A_c = (VK + \gamma I)^{-1} V1_l \\
        A_s = (VK + \gamma I)^{-1} \Phi_s,\quad s = 1, \dots, n 
    \end{gathered}
\end{equation*}

In this case, $\gamma$ controls the amount of regularization applied so that the resulting matrix is not singular.

The values of $c$ and the $m$ coefficients $\mu_s$ can be obtained solving the following system of equations:

\begin{equation*}
    \begin{gathered}
        c [1_l^T VKA_c - 1_l^T V 1_l] + \sum_{s=1}^m \mu_s [1_l^T VKA_s - 1_l^T \Phi_s] = [1_l^T VKA_V - 1_l^T V Y] \\
        c [A_c^TK\Phi_k - 1_l^T\Phi_k] + \sum_{s=1}^m \mu_s A_s^T K \Phi_k = [A_V^T K \Phi_k - Y^T \Phi_k],\quad k=1, \dots, m
    \end{gathered}
\end{equation*}

This algorithm, which uses $m$ invariants, is called vSVM\&$\text{I}_m$ if the $V$-matrix is used
or SVM\&$\text{I}_m$ in case it is not.

\section{Overview of the LUSI algorithm}

Following the mathematical formulations from the previous sections, let us now present an overview of
the LUSI algorithm.

Consider the following learning method given a training sample $(x_i, y_i),\; i=1, \dots, l$ and
$\psi_k(x),\; k=1, \dots, m$ predicates:

\begin{enumerate}[label=\textbf{Step \arabic*:}]
    \item Construct SVM or vSVM estimate of conditional probability as described in section
    \ref{sect:solvin_learning_problem}, without considering the predicates.
    \item Find the maximal disagreement value $\Tau_s$ as defined in \eqref{eq:predicate_selection}
    for vectors
    \[
        \Phi_k = (\psi_k(x_1), \dots, \psi_k(x_l))^T,\quad k=1, \dots, m
    \]
    \item If $\Tau_s > \delta$, add the invariant associated to the predicate $\psi_s$; otherwise stop.
    \item Find a new approximation of the conditional probability function and go back to \textbf{Step 2};
    otherwise stop.
\end{enumerate}

\section{Main results and limitations}

According to the original work, LUSI yields quiet good results overall, reducing the error and thus improving
the accuracy of the models that use it compared to models that do not use statistical invariants. Moreover,
the authors state that it can reduce the number of necessary training examples to obtain a good approximation
of the conditional probability function. Therefore, this method could be very useful in cases in which the
amount of available training data is small.

However, this new learning paradigm presents some important flaws:

\begin{enumerate}
    \item The selected invariants are problem dependent, which means that it is hard to have general invariants
    that can both be applied to multiple problems without requiring any kind of previous knowledge
    and yield good results.
    \item Often, the invariant selection is a ``black-art'' due to the fact that they can either be
    very esoteric or require a lot of knowledge about the specific problem which might be hard
    to obtain. Hence, some craftsmanship is required when selecting the invariants that are going
    to be used.
    \item Considering expressions \eqref{eq:invariant_approximation} and \eqref{eq:predicate_selection}, we
    can clearly see that the invariants can only consider the statistical information of
    the positive class. The way that the values are computed make it hinder the application of the invariants
    to the negative class and to multiclass classification problems since there is no positive class in this kind
    of scenarios. This a very serious drawback of this method that needs to be further addressed.
\end{enumerate}

Thus, even though the use of invariants can improve the obtained results, they also introduce some additional
complexity to the task because they require extra knowledge that may not be accessible.
